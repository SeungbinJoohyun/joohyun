{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4c6e467b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from sqlalchemy import create_engine, inspect\n",
    "\n",
    "# 헤더 설정 (기존 headers 변수 사용)\n",
    "# 대량 크롤링 시 IP 차단당해서 Client Error가 발생할 수 있으니 주기적으로 IP를 변경해주기\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/137.0.7151.104 Safari/537.36'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3b77a9eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymysql in /opt/anaconda3/lib/python3.12/site-packages (1.1.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install pymysql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f380ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "user = 'joohyun'\n",
    "password = 'joohyun0805'\n",
    "host = '13.125.208.76'\n",
    "port = '3306'\n",
    "database = 'kbo_preview_database'\n",
    "\n",
    "myngine = create_engine(f'mysql+pymysql://{user}:{password}@{host}:{port}/{database}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1ece99f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "selectsql=\"\"\"\n",
    "select max(s_no) as max_sno\n",
    "from(\n",
    "\tselect c.s_no as s_no, \n",
    "\tc.`경기 날짜` as play_date,\n",
    "\tc.`원정 선발투수` as away_pitcher,\n",
    "\tc.`홈 선발투수` as home_pitcher,\n",
    "\td.p_no as home_no,\n",
    "\tc.away_no\n",
    "\tfrom (\n",
    "\t\tselect a.*, \n",
    "\t\t\tb.p_no as away_no\n",
    "\t\tfrom kbo_preview_data a \n",
    "\t\tleft join kbo_pitcher_data b\n",
    "\t\ton a.`원정 선발투수` = b.p_name\t\n",
    "\t) c\n",
    "\tleft join kbo_pitcher_data d\n",
    "\ton c.`홈 선발투수` = d.p_name\n",
    ")e\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "076bacdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20250345"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masno= pd.read_sql_query(sql=selectsql, con=myngine)\n",
    "masno.max_sno.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6313acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_no = masno.max_sno.values[0] + 1\n",
    "END_NO = masno.max_sno.values[0] + 5\n",
    "\n",
    "\n",
    "# Funtion1: 단일 게임 데이터를 크롤링\n",
    "def crawl_single_game_data(s_no):\n",
    "    url = f\"https://statiz.sporki.com/schedule/?m=preview&s_no={s_no}\"\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # total_score = crawl_score_from_soup(soup)\n",
    "        pitchers = crawl_pitcher_from_soup(soup)\n",
    "        game_date = crawl_date_from_soup(soup)\n",
    "        \n",
    "        return {\n",
    "            's_no': s_no,\n",
    "            'play_date': game_date,\n",
    "            # '총득점': total_score,\n",
    "            'home_pitcher': pitchers[0] if len(pitchers) > 0 else None,\n",
    "            'away_pitcher': pitchers[1] if len(pitchers) > 1 else None\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"s_no {s_no} 크롤링 실패: {e}\")\n",
    "        return None\n",
    "\n",
    "def crawl_pitcher_from_soup(soup):\n",
    "    try:\n",
    "        pitchers_name = [div.get_text(strip=True) for div in soup.find_all('div', style=\"margin-top:.3rem;\")]\n",
    "        return pitchers_name\n",
    "    except Exception as e:\n",
    "        print(f\"투수 정보 크롤링 오류: {e}\")\n",
    "        return []\n",
    "\n",
    "# Function4: 경기 날짜 크롤링 \n",
    "def crawl_date_from_soup(soup):\n",
    "    try:\n",
    "        text = soup.select_one('div.txt').get_text(strip=True)\n",
    "        date_data = re.search(r'\\d{2}-\\d{2}', text).group()\n",
    "        return date_data\n",
    "    except Exception as e:\n",
    "        print(f\"날짜 크롤링 오류: {e}\")\n",
    "        return None\n",
    "\n",
    "# 전체 데이터 크롤링 함수 (DataFrame append 방식)\n",
    "def crawl_all_games():\n",
    "    # 빈 DataFrame 초기화\n",
    "    merged_df = pd.DataFrame(columns=['s_no', 'play_date', 'home_pitcher', 'away_pitcher'])\n",
    "    \n",
    "    for s_no in tqdm(range(start_no, END_NO + 1), desc=\"크롤링 진행\"):\n",
    "        game_data = crawl_single_game_data(s_no)\n",
    "        if game_data:\n",
    "            # 개별 게임 데이터를 DataFrame으로 변환\n",
    "            single_game_df = pd.DataFrame([game_data])\n",
    "            # 전체 DataFrame에 append\n",
    "            merged_df = pd.concat([merged_df, single_game_df], ignore_index=True)\n",
    "            print(f\"s_no {s_no} 추가 완료 (총 {len(merged_df)}개)\")\n",
    "        \n",
    "        # 서버 부하 방지를 위한 딜레이\n",
    "        # 초기에 0.5로 설정하니까 웹사이트 내부에서 비정상적인 행동 패턴으로 파악하고 크롤링을 차단해버림\n",
    "        time.sleep(random.uniform(1.5, 3.0))\n",
    "    \n",
    "    return merged_df\n",
    "\n",
    "# 에러 처리 및 재시도 기능 포함 크롤링 (DataFrame append 방식)\n",
    "def crawl_with_retry(start_no=start_no, end_no=END_NO, max_retries=3):\n",
    "    # 빈 DataFrame 초기화\n",
    "    merged_df = pd.DataFrame(columns=['s_no', 'play_date', 'home_pitcher', 'away_pitcher'])\n",
    "    failed_s_nos = []\n",
    "    \n",
    "    for s_no in tqdm(range(start_no, end_no + 1), desc=\"크롤링 진행\"):\n",
    "        success = False\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                game_data = crawl_single_game_data(s_no)\n",
    "                if game_data:\n",
    "                    # 개별 게임 데이터를 DataFrame으로 변환\n",
    "                    single_game_df = pd.DataFrame([game_data])\n",
    "                    # 전체 DataFrame에 append\n",
    "                    merged_df = pd.concat([merged_df, single_game_df], ignore_index=True)\n",
    "                    print(f\"s_no {s_no} 추가 완료 (총 {len(merged_df)}개)\")\n",
    "                    success = True\n",
    "                    break\n",
    "                else:\n",
    "                    time.sleep(1)  # 실패 시 잠시 대기\n",
    "            except Exception as e:\n",
    "                if attempt == max_retries - 1:\n",
    "                    print(f\"s_no {s_no} 최종 실패: {e}\")\n",
    "                    failed_s_nos.append(s_no)\n",
    "                else:\n",
    "                    time.sleep(2)  # 재시도 전 더 긴 대기\n",
    "        if success:\n",
    "            time.sleep(0.5)  # 성공 시 기본 대기\n",
    "    if failed_s_nos:\n",
    "        print(f\"실패한 s_no 목록: {failed_s_nos}\")\n",
    "    \n",
    "    return merged_df, failed_s_nos\n",
    "\n",
    "# 중간 저장 기능 포함 크롤링 (DataFrame append 방식, 추천)\n",
    "def crawl_with_checkpoint(start_no=start_no, end_no=END_NO, checkpoint_interval=50):\n",
    "    # 빈 DataFrame 초기화\n",
    "    merged_df = pd.DataFrame(columns=['s_no', 'play_date', 'home_pitcher', 'away_pitcher'])\n",
    "    \n",
    "    for i in range(start_no, end_no + 1, checkpoint_interval):\n",
    "        batch_end = min(i + checkpoint_interval - 1, end_no)\n",
    "        for s_no in tqdm(range(i, batch_end + 1), desc=f\"배치 {i}-{batch_end}\"):\n",
    "            game_data = crawl_single_game_data(s_no)\n",
    "            if game_data:\n",
    "                # 개별 게임 데이터를 DataFrame으로 변환\n",
    "                single_game_df = pd.DataFrame([game_data])\n",
    "                # 전체 DataFrame에 append\n",
    "                merged_df = pd.concat([merged_df, single_game_df], ignore_index=True)\n",
    "            time.sleep(0.5)\n",
    "        # 중간 저장 (백업용)\n",
    "        checkpoint_file = f'checkpoint_baseball_data_{start_no}_games.csv'\n",
    "        merged_df.to_csv(checkpoint_file, index=False, encoding='utf-8-sig')\n",
    "    \n",
    "    return merged_df\n",
    "\n",
    "# 실패한 s_no들에 대해서만 재크롤링 (DataFrame append 방식)\n",
    "def retry_failed_games(failed_s_nos, max_retries=5):\n",
    "    if not failed_s_nos:\n",
    "        print(\"재크롤링할 데이터가 없습니다.\")\n",
    "        return pd.DataFrame(), []\n",
    "    # 빈 DataFrame 초기화\n",
    "    retry_df = pd.DataFrame(columns=['s_no', 'play_date', 'home_pitcher', 'away_pitcher'])\n",
    "    still_failed = []\n",
    "    \n",
    "    for s_no in tqdm(failed_s_nos, desc=\"재크롤링\"):\n",
    "        success = False\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                game_data = crawl_single_game_data(s_no)\n",
    "                if game_data:\n",
    "                    # 개별 게임 데이터를 DataFrame으로 변환\n",
    "                    single_game_df = pd.DataFrame([game_data])\n",
    "                    # 재시도 DataFrame에 append\n",
    "                    retry_df = pd.concat([retry_df, single_game_df], ignore_index=True)\n",
    "                    success = True\n",
    "                    break\n",
    "                else:\n",
    "                    time.sleep(2)\n",
    "            except Exception as e:\n",
    "                time.sleep(3)\n",
    "        if not success:\n",
    "            still_failed.append(s_no)\n",
    "    if still_failed:\n",
    "        print(f\"여전히 실패한 s_no: {still_failed}\")\n",
    "    \n",
    "    return retry_df, still_failed\n",
    "\n",
    "\n",
    "merged_df = crawl_with_checkpoint(start_no, END_NO, checkpoint_interval=50)\n",
    "merged_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b498b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pitchers_df = pd.read_csv('pitcher_no.csv')\n",
    "pitchers_df = pd.DataFrame(pitchers_df)\n",
    "pitchers_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "27c1b77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_df = pd.merge(\n",
    "  merged_df,\n",
    "  pitchers_df,\n",
    "  left_on='home_pitcher',\n",
    "  right_on='p_name',\n",
    "  how='left'\n",
    ").rename(columns={'p_no' : 'home_pitcher_no'}).drop(columns=['p_name'])\n",
    "\n",
    "test_data_df = pd.merge(\n",
    "  test_data_df,\n",
    "  pitchers_df,\n",
    "  left_on='away_pitcher',\n",
    "  right_on='p_name',\n",
    "  how='left'\n",
    ").rename(columns={'p_no': 'away_pitcher_no'}).drop(columns=['p_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "617f8407",
   "metadata": {},
   "outputs": [],
   "source": [
    "emptyDf1=pd.DataFrame()\n",
    "emptyDf2=pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6d7ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_nodata ='https://statiz.sporki.com/stats/?m=team&m2=pitching'\n",
    "def crawl_statiz_pitcher_data(url):\n",
    "    \"\"\"\n",
    "    스탯티즈에서 선발투수 등판 기록을 크롤링하는 함수\n",
    "    \"\"\"\n",
    "    tempValue = ''     #rowspan이 2로 행이 묶여 정보가 누락되는 문제 해결을 위한 변수들\n",
    "    onRowapply1 = False #한 행의 열데이터 들을 수집하면서 rowspan==2를 감지하면 True\n",
    "    onRowapply2 = False #한 행의 열데이터를 모두 수집하였을때 onRowapply1이 True면 True\n",
    "    j=0\n",
    "    try:\n",
    "        \n",
    "        # HTTP 요청 헤더 설정 (봇 차단 방지)\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/137.0.7151.104 Safari/537.36'\n",
    "        }\n",
    "        \n",
    "        # 웹페이지 요청\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # BeautifulSoup 객체 생성\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # 테이블 찾기 (일반적으로 통계 데이터는 table 태그 안에 있음)\n",
    "        table = soup.find('table')\n",
    "        if not table:\n",
    "            print(\"테이블을 찾을 수 없습니다.\")\n",
    "            return None\n",
    "        \n",
    "        # 헤더 추출 (th 태그 또는 첫 번째 tr의 td 태그)\n",
    "        headers = []\n",
    "        header_row = table.find('tr')\n",
    "        if header_row:\n",
    "            header_cells = header_row.find_all(['th', 'td'])\n",
    "            headers = [cell.get_text(strip=True) for cell in header_cells]\n",
    "        \n",
    "        # 데이터 행 추출\n",
    "        data_rows = []\n",
    "        rows = table.find_all('tr')[1:]  # 첫 번째 행(헤더) 제외\n",
    "        for i in range (len(list(rows))-1): #마지막 통산 기록 부분 제외를 위해 -1\n",
    "            row = rows[i]\n",
    "            cells = row.find_all('td')\n",
    "            if cells:  # td 태그가 있는 행만 처리\n",
    "                row_data = []\n",
    "                \n",
    "                while j <= (len(list(cells))-1):\n",
    "                    cell = cells[j]\n",
    "                    if onRowapply2 ==True:\n",
    "                        text= tempValue\n",
    "                        onRowapply1 = False\n",
    "                        onRowapply2 = False\n",
    "                        j-=1 #rowspan==2로 인해 누락된 데이터를 입력하느라 원래 데이터를 누락하지 않도록 함\n",
    "                    else:\n",
    "                        # 셀 내용 추출 (줄바꿈 제거, 공백 정리)\n",
    "                        text = cell.get_text(separator=' ', strip=True)\n",
    "\n",
    "                    # 추가 줄바꿈 문자 제거\n",
    "                    text = text.replace('\\n', ' ').replace('\\r', '').strip()\n",
    "                    # 연속된 공백을 하나로 통일\n",
    "                    text = ' '.join(text.split())\n",
    "                    row_data.append(text)\n",
    "                    # 첫번째 컬럼의 td row span 속성값이 2이상일 경우 값을 저장하고 onRowapply1을 True로 변경\n",
    "                    if i==1 and j==0:\n",
    "                        tmpRow = cell.attrs[\"rowspan\"]\n",
    "                        if int( cell[\"rowspan\"] ) >= 2:\n",
    "                            tempValue = text\n",
    "                            onRowapply1 = True #rowspan==2를 감지하면 True로 변경\n",
    "                    j+=1\n",
    "                j=0\n",
    "                \n",
    "                if row_data:  # 빈 행이 아닌 경우만 추가\n",
    "                    data_rows.append(row_data)\n",
    "            if onRowapply1== True: #누락된 데이터를 바로 다음 열에 입력하지 않도록 onRowapply2를 이용\n",
    "                onRowapply2= True\n",
    "        \n",
    "        return headers, data_rows\n",
    "        \n",
    "    except requests.RequestException as e:\n",
    "        print(f\"웹페이지 요청 중 오류 발생: {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"데이터 추출 중 오류 발생: {e}\")\n",
    "        return None\n",
    "def crawl_statiz_pitcher_nodata(url_nodata):\n",
    "    \"\"\"\n",
    "    결측치 처리를 위한 함수\n",
    "    스탯티즈에서 선발투수 등판 기록이 없을때 리그 평균 스탯을 크롤링하는 함수\n",
    "    \"\"\"\n",
    "    try:\n",
    "        headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/137.0.7151.104 Safari/537.36'\n",
    "        }\n",
    "\n",
    "        # 웹페이지 요청\n",
    "        response = requests.get(url_nodata, headers=headers)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        # BeautifulSoup 객체 생성\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # 테이블 찾기 (일반적으로 통계 데이터는 table 태그 안에 있음)\n",
    "        table = soup.find('table')\n",
    "        if not table:\n",
    "            print(\"테이블을 찾을 수 없습니다.\")\n",
    "            return None\n",
    "        #colum데이터 추출\n",
    "        headers1 = []\n",
    "        header_row1 = table.find('tr')\n",
    "        if header_row1:\n",
    "            header_cells1 = header_row1.find_all([ 'th','td'])\n",
    "            headers1 = [cell.get_text(strip=True) for cell in header_cells1]\n",
    "        #league_colum_DF = pd.DataFrame(headers1)\n",
    "        # 리그 시즌기록 데이터 추출 (tbody태그 내부에 데이터 존재)\n",
    "        headers2 = []\n",
    "        header_row2 = table.find('tbody')\n",
    "        if header_row2:\n",
    "            header_cells2 = header_row2.find_all([ 'th','td'])\n",
    "            headers2 = [cell.get_text(strip=True) for cell in header_cells2]\n",
    "        #nostat_pitcher_DF = nostat_pitcher_DF.transpose()\n",
    "\n",
    "\n",
    "        return headers1,headers2\n",
    "\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"웹페이지 요청 중 오류 발생: {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"데이터 추출 중 오류 발생: {e}\")\n",
    "        return None\n",
    "\n",
    "columlist = ['ERA', 'FIP', 'WHIP']\n",
    "columlist1 = ['ERA1', 'FIP1', 'WHIP1']\n",
    "columlist2 = ['ERA2', 'FIP2', 'WHIP2']\n",
    "\n",
    "picher_stat10_list = []\n",
    "\n",
    "for i in range(5):\n",
    "    picher_stat2_list=[]\n",
    "    #원정 투수 스탯\n",
    "    \n",
    "    player_id=test_data_df.home_pitcher_no.values[i]\n",
    "    url = f\"https://statiz.sporki.com/player/?m=year&p_no={player_id}\"\n",
    "    picher_data=crawl_statiz_pitcher_data(url)\n",
    "\n",
    "    if picher_data[1]==[]:\n",
    "        picher_nodata=crawl_statiz_pitcher_nodata(url_nodata)\n",
    "        picher_nodata_DF=pd.DataFrame(picher_nodata[1])\n",
    "        picher_nodata_DF = picher_nodata_DF.set_axis(picher_nodata[0], axis=0).transpose()\n",
    "        picher_stat_DF = picher_nodata_DF\n",
    "\n",
    "    else:\n",
    "        picherdata_DF=pd.DataFrame(picher_data[1])\n",
    "        picher_data_DF = picherdata_DF.set_axis(picher_data[0], axis=1)\n",
    "        picher_stat_DF = picher_data_DF\n",
    "    \n",
    "    picher_stat_DF.iloc[-1]\n",
    "    picher_stat_DF = pd.DataFrame(picher_stat_DF.iloc[-1])\n",
    "    picher_stat_DF = picher_stat_DF.transpose()\n",
    "\n",
    "    picher_stat_DF = picher_stat_DF[columlist]\n",
    "    picher_stat_DF.columns = columlist1\n",
    "\n",
    "    emptyDf1 = pd.concat( [emptyDf1, picher_stat_DF] )\n",
    "\n",
    "    player_id=test_data_df.away_pitcher_no.values[i]\n",
    "    url = f\"https://statiz.sporki.com/player/?m=year&p_no={player_id}\"\n",
    "    picher_data=crawl_statiz_pitcher_data(url)\n",
    "\n",
    "    if picher_data[1]==[]:\n",
    "        picher_nodata=crawl_statiz_pitcher_nodata(url_nodata)\n",
    "        picher_nodata_DF=pd.DataFrame(picher_nodata[1])\n",
    "        picher_nodata_DF = picher_nodata_DF.set_axis(picher_nodata[0], axis=0).transpose()\n",
    "        picher_stat_DF = picher_nodata_DF\n",
    "\n",
    "    else:\n",
    "        picherdata_DF=pd.DataFrame(picher_data[1])\n",
    "        picher_data_DF = picherdata_DF.set_axis(picher_data[0], axis=1)\n",
    "        picher_stat_DF = picher_data_DF\n",
    "    \n",
    "    picher_stat_DF.iloc[-1]\n",
    "    picher_stat_DF = pd.DataFrame(picher_stat_DF.iloc[-1])\n",
    "    picher_stat_DF = picher_stat_DF.transpose()\n",
    "\n",
    "    picher_stat_DF = picher_stat_DF[columlist]\n",
    "    picher_stat_DF.columns = columlist2\n",
    "\n",
    "    \n",
    "    emptyDf2 = pd.concat( [emptyDf2, picher_stat_DF], axis=0 )\n",
    "    \n",
    "    # picher_stat2_list.append(picher_stat_DF)\n",
    "    # print(picher_stat2_list)\n",
    "    \n",
    "    # picher_stat10_list.append(picher_stat2_list)\n",
    "    # print(picher_stat10_list)\n",
    "emptyDf1.reset_index(drop=True, inplace=True)\n",
    "emptyDf2.reset_index(drop=True, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "733bd571",
   "metadata": {},
   "outputs": [],
   "source": [
    "picher10_stat_DF=pd.concat( [emptyDf1, emptyDf2], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dcdc3753",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_column_list = [\"s_no\", \"ERA1\", \"FIP1\", \"WHIP1\", \"ERA2\", \"FIP2\", \"WHIP2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "11a874b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tempDF = pd.concat([test_data_df, picher10_stat_DF], axis=1)\n",
    "test_DF=tempDF[test_column_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bbe39475",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_DF.to_csv('testdata.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
